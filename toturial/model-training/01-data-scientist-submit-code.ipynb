{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0",
   "metadata": {},
   "source": [
    "# third party\n",
    "import jax\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# syft absolute\n",
    "import syft as sy"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. DS logins to the domain with the credentials created by the DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2",
   "metadata": {},
   "source": [
    "node = sy.orchestra.launch(name=\"esol-domain\", dev_mode=True)\n",
    "ds_client = node.login(email=\"sheldon@caltech.edu\", password=\"changethis\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Inspect the datasets on the domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4",
   "metadata": {},
   "source": [
    "datasets = ds_client.datasets.get_all()\n",
    "assert len(datasets) == 1\n",
    "datasets"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5",
   "metadata": {},
   "source": [
    "assets = datasets[0].assets\n",
    "assert len(assets) == 5\n",
    "assets"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6",
   "metadata": {},
   "source": [
    "y = assets[0]\n",
    "#y"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7",
   "metadata": {},
   "source": [
    "x = assets[1]\n",
    "#x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8e642bd-bee4-43b9-8755-1973ebc837a7",
   "metadata": {},
   "source": [
    "x_masks = assets[2]\n",
    "#x_masks"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b951114a-6c39-427f-a1be-9dd342d5b9e0",
   "metadata": {},
   "source": [
    "edge_index = assets[3]\n",
    "#edge_index"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ef633e-a61d-4fed-a8ba-0e8ac07d1912",
   "metadata": {},
   "source": [
    "edge_index_masks = assets[4]\n",
    "edge_index_masks"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### The DS can not access the real data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9",
   "metadata": {},
   "source": [
    "assert x.data is None\n",
    "assert edge_index.data is None\n",
    "assert y.data is None"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### The DS can only access the mock data, which is some random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11",
   "metadata": {},
   "source": [
    "x_mock = x.mock\n",
    "edge_index_mock = edge_index.mock\n",
    "y_mock = y.mock\n",
    "print(x_mock.shape)\n",
    "print(edge_index_mock.shape)\n",
    "print(y_mock.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### We need the pointers to the mock data to construct a `syft` function (later in the notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13",
   "metadata": {},
   "source": [
    "x_mock_ptr = x.pointer\n",
    "#x_mock_ptr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14",
   "metadata": {},
   "source": [
    "type(x_mock)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9e4b39b-a90e-4cdf-97cd-febcc5030067",
   "metadata": {},
   "source": [
    "edge_index_mock = edge_index.mock\n",
    "edge_index_mock_ptr = edge_index.pointer\n",
    "#edge_index_mock_ptr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15",
   "metadata": {},
   "source": [
    "y_mock = y.mock\n",
    "y_mock_ptr = y.pointer\n",
    "#y_mock_ptr"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5c6676-6e9b-45f7-a681-fa6b6b696ff9",
   "metadata": {},
   "source": [
    "# @todo: recreate the DATA as it was in the 00-data-owner-upload-dat... then change \"in_channels=x.shape[-1],\" to \"in_channels=DATA.num_features,\""
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 2. The DS prepare the training code and experiment on the mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c9fb09d-9db8-41d0-97f9-a438f61028e6",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F \n",
    "from torch_geometric.nn import GCNConv, TopKPooling, global_mean_pool\n",
    "from torch_geometric.nn import global_mean_pool as gap, global_max_pool as gmp\n",
    "\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(42)\n",
    "\n",
    "        self.initial_conv = GCNConv( # The graph convolutional operator from the “Semi-supervised Classification with Graph Convolutional Networks” paper\n",
    "          in_channels=x_mock.shape[-1], # number of features per node of graph before transformation\n",
    "          out_channels=EMBEDDING_DIM # number of features per node of graph after transformation\n",
    "        )\n",
    "        self.conv1 = GCNConv(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "        self.conv2 = GCNConv(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "        self.conv3 = GCNConv(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "        self.out = Linear(\n",
    "          in_features=EMBEDDING_DIM*2, # we stack the different global pooling aggregations below\n",
    "          out_features=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, batch_index):\n",
    "        # First Conv layer\n",
    "        hidden = self.initial_conv(x, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "\n",
    "        # Other Conv layers\n",
    "        hidden = self.conv1(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv2(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "        hidden = self.conv3(hidden, edge_index)\n",
    "        hidden = F.tanh(hidden)\n",
    "          \n",
    "        # Global Pooling (stack different aggregations over nodes of graph)\n",
    "        hidden = torch.cat([gmp(hidden, batch_index), \n",
    "                            gap(hidden, batch_index)], dim=1)\n",
    "\n",
    "        # Apply a final (linear) classifier.\n",
    "        out = self.out(hidden)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "MODEL = GCN()\n",
    "print(MODEL)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in MODEL.parameters()))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3ffba41-b5e8-4be5-b8a2-f760fc6c79ec",
   "metadata": {},
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "NUM_GRAPHS_PER_BATCH = 64\n",
    "\n",
    "# Use GPU for training (if available)\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e225fc81-7e91-4d7d-84e7-f1fc9415f3b6",
   "metadata": {},
   "source": [
    "DATA_ = {\n",
    " \"x\": x,\n",
    " \"edge_index\": edge_index,\n",
    " \"y\": y,\n",
    "}\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "524e2199-d5dd-40bb-b7de-472b643b9497",
   "metadata": {},
   "source": [
    "def train(model, data):\n",
    "  model = model.to(DEVICE)\n",
    "\n",
    "  loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0007)\n",
    "\n",
    "  data_size = x.shape[0]\n",
    "  train_loader = DataLoader(\n",
    "    data[:int(data_size * 0.8)], \n",
    "    batch_size=NUM_GRAPHS_PER_BATCH, \n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  for batch in train_loader:\n",
    "    # Use GPU\n",
    "    batch.to(DEVICE)  \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad() \n",
    "    # Passing the node features and the edge info\n",
    "    pred, embedding = model(batch.x.float(), batch.edge_index, batch.batch) \n",
    "    # Calculating the loss and gradients\n",
    "    loss = loss_fn(pred, batch.y)     \n",
    "    loss.backward()  \n",
    "    # Update using the gradients\n",
    "    optimizer.step()   \n",
    "  return loss, embedding\n",
    "\n",
    "def train_wrapper():\n",
    "  print(\"Starting training...\")\n",
    "  losses = []\n",
    "  for epoch in range(2000): \n",
    "      loss, h = train(MODEL, DATA_)\n",
    "      losses.append(loss)\n",
    "      if epoch % 100 == 0: \n",
    "        print(f\"Epoch {epoch} | Train Loss {loss}\")\n",
    "  return losses \n",
    "\n",
    "LOSSES = train_wrapper()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "45c4a010-efad-40b7-ab07-d26102683a38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c29f07-110c-46bd-8653-6792d9c16904",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f185ac75-a8c5-42ca-9301-11a464670b08",
   "metadata": {},
   "source": [],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "source": [
    "train_accs, params = mnist_3_linear_layers(\n",
    "    mnist_images=mock_images, mnist_labels=mock_labels\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Inspect the training accuracies and the shape of the model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "source": [
    "train_accs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "source": [
    "jax.tree_map(lambda x: x.shape, params)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 3. Now that the code works on mock data, the DS submits the code request for execution to the DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### First the DS wraps the training function with the `@sy.syft_function` decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "source": [
    "@sy.syft_function(\n",
    "    input_policy=sy.ExactMatch(\n",
    "        mnist_images=mock_images_ptr, mnist_labels=mock_labels_ptr\n",
    "    ),\n",
    "    output_policy=sy.SingleExecutionExactOutput(),\n",
    ")\n",
    "def mnist_3_linear_layers(mnist_images, mnist_labels):\n",
    "    # import the packages\n",
    "    # stdlib\n",
    "    import itertools\n",
    "    import time\n",
    "\n",
    "    # third party\n",
    "    from jax import grad\n",
    "    from jax import jit\n",
    "    from jax import random\n",
    "    from jax.example_libraries import optimizers\n",
    "    from jax.example_libraries import stax\n",
    "    from jax.example_libraries.stax import Dense\n",
    "    from jax.example_libraries.stax import LogSoftmax\n",
    "    from jax.example_libraries.stax import Relu\n",
    "    import jax.numpy as jnp\n",
    "    import numpy.random as npr\n",
    "\n",
    "    # define the neural network\n",
    "    init_random_params, predict = stax.serial(\n",
    "        Dense(1024), Relu, Dense(1024), Relu, Dense(10), LogSoftmax\n",
    "    )\n",
    "\n",
    "    # initialize the random parameters\n",
    "    rng = random.PRNGKey(0)\n",
    "    _, init_params = init_random_params(rng, (-1, 784))\n",
    "\n",
    "    # the hyper parameters\n",
    "    num_epochs = 10\n",
    "    batch_size = 4\n",
    "    num_train = mnist_images.shape[0]\n",
    "    num_complete_batches, leftover = divmod(num_train, batch_size)\n",
    "    num_batches = num_complete_batches + bool(leftover)\n",
    "    step_size = 0.001\n",
    "    momentum_mass = 0.9\n",
    "\n",
    "    # initialize the optimizer\n",
    "    opt_init, opt_update, get_params = optimizers.momentum(\n",
    "        step_size, mass=momentum_mass\n",
    "    )\n",
    "    opt_state = opt_init(init_params)\n",
    "    itercount = itertools.count()\n",
    "\n",
    "    @jit\n",
    "    def update(i, opt_state, batch):\n",
    "        params = get_params(opt_state)\n",
    "        return opt_update(i, grad(loss)(params, batch), opt_state)\n",
    "\n",
    "    def data_stream():\n",
    "        \"\"\"\n",
    "        Create a batch of data picked randomly\n",
    "        \"\"\"\n",
    "        rng = npr.RandomState(0)\n",
    "        while True:\n",
    "            perm = rng.permutation(num_train)\n",
    "            for i in range(num_batches):\n",
    "                batch_idx = perm[i * batch_size : (i + 1) * batch_size]\n",
    "                yield mnist_images[batch_idx], mnist_labels[batch_idx]\n",
    "\n",
    "    def loss(params, batch):\n",
    "        inputs, targets = batch\n",
    "        preds = predict(params, inputs)\n",
    "        return -jnp.mean(jnp.sum(preds * targets, axis=1))\n",
    "\n",
    "    def accuracy(params, batch):\n",
    "        inputs, targets = batch\n",
    "        target_class = jnp.argmax(targets, axis=1)\n",
    "        predicted_class = jnp.argmax(predict(params, inputs), axis=1)\n",
    "        return jnp.mean(predicted_class == target_class)\n",
    "\n",
    "    batches = data_stream()\n",
    "    train_accs = []\n",
    "    print(\"\\nStarting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        for _ in range(num_batches):\n",
    "            opt_state = update(next(itercount), opt_state, next(batches))\n",
    "        epoch_time = time.time() - start_time\n",
    "        params = get_params(opt_state)\n",
    "        train_acc = accuracy(params, (mnist_images, mnist_labels))\n",
    "        print(f\"Epoch {epoch} in {epoch_time:0.2f} sec\")\n",
    "        print(f\"Training set accuracy {train_acc}\")\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "    return train_accs, params"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "#### Then the DS creates a new project with relevant name and description, as well as specify itself as a member of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "source": [
    "new_project = sy.Project(\n",
    "    name=\"Training a 3-layer jax neural network on MNIST data\",\n",
    "    description=\"\"\"Hi, I would like to train my neural network on your MNIST data \n",
    "                (I can download it online too but I just want to use Syft coz it's cool)\"\"\",\n",
    "    members=[ds_client],\n",
    ")\n",
    "new_project"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Add a code request to the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "source": [
    "new_project.create_code_request(obj=mnist_3_linear_layers, client=ds_client)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "source": [
    "ds_client.code"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### Start the project which will notifies the DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "source": [
    "project = new_project.send()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "source": [
    "project.events"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "source": [
    "project.requests"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "source": [
    "project.requests[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### 📓 Now switch to the [second DO's notebook](./02-data-owner-review-approve-code.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
